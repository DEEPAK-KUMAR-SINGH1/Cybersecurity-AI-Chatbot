import os
import streamlit as st
from dotenv import load_dotenv
from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from pathlib import Path


# Load API key
#os.environ["MISTRAL_API_KEY"] = 
os.environ["MISTRAL_API_KEY"] = st.secrets["MISTRAL_API_KEY"]

# LLM Setup
llm = ChatMistralAI(
    model="mistral-small",
    temperature=0.3
)

# Avatars
avatar_path = Path("ai_image.png")
user_avatar_path = Path("my image.png")

# Prompt template
template = """You are a friendly , professional and your name is CyberYodha also u have to introduce yourself to user firstly **Cybersecurity AI Assistant** ğŸ¤–.  

ğŸ¯ **Rules for replying:**  
1. If the user says only "hi", "hello", or small talk â†’ reply briefly and friendly.  
   Example: "Hi! ğŸ‘‹ How can I help you with a cybersecurity question today?"  

2. If the user asks a **concept/definition** â†’  
   - Explain in **simple, easy-to-understand words**.  
   - Use **bullet points** or a short example.  

3. If the user describes a **problem or issue** â†’  
   - First, ask clarifying questions:  
     â€¢ "Can you tell me what kind of user you are? (student, beginner, professional, startup, etc.)"  
     â€¢ "Whatâ€™s the main problem youâ€™re facing?"  
   - Then provide:  
     â€¢ A **clear explanation** of the issue.  
     â€¢ A **step-by-step practical solution**.  

4. If the query is **technical** â†’  
   - Give a **short explanation** with a real-world analogy or example.  
   - Then provide the **solution or steps** to fix it.  

ğŸ’¡ Always keep the tone:  
- Friendly, supportive, and easy-to-understand.  
- Avoid jargon unless necessary (and explain it when used).  

---

ğŸ“œ **Chat history**:  
{history}  

ğŸ‘¤ **User**: {input}  
ğŸ¤– **AI**:"""
prompt = PromptTemplate(input_variables=["history", "input"], template=template)


# Setup memory (new API)
if "store" not in st.session_state:
    st.session_state.store = {}

def get_session_history(session_id: str):
    if session_id not in st.session_state.store:
        st.session_state.store[session_id] = InMemoryChatMessageHistory()
    return st.session_state.store[session_id]


# RunnableWithMessageHistory wrapper
chain = prompt | llm
chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history"
)

# Streamlit UI
st.set_page_config(page_title="Cybersecurity AI Chatbot", page_icon="ğŸ¤–")
st.title("ğŸ” Cybersecurity AI Chatbot")
st.write("Ask me anything about cybersecurity in **simple terms**!")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
user_input = st.chat_input("Type your cybersecurity question here...")

if user_input:
    # Show user message
    st.chat_message("user", avatar=str(user_avatar_path)).markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Invoke new chain API
    response = chain_with_history.invoke(
        {"input": user_input},
        config={"configurable": {"session_id": "user-session"}}
    )

    # Show AI response
    with st.chat_message("assistant", avatar=str(avatar_path)):
        st.markdown(response.content)

    # Save response
    st.session_state.messages.append({"role": "assistant", "content": response.content})



